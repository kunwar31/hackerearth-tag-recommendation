{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import hamming_loss, f1_score\n",
    "from keras.optimizers import SGD,RMSprop,Adam\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import label_ranking_average_precision_score, coverage_error, label_ranking_loss\n",
    "import pickle\n",
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from keras.losses import binary_crossentropy,mean_squared_error\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "np.random.seed(31)\n",
    "tf.set_random_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1612145717016bab232c39d84682fde3211ce5b8"
   },
   "outputs": [],
   "source": [
    "CLEAN_DF_BASE = ''\n",
    "EMB_BASE = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv( CLEAN_DF_BASE + 'cleaned_train.csv')\n",
    "df_test = pd.read_csv( CLEAN_DF_BASE + 'cleaned_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad1e9fdf97f3291a7d1797b25f0c7a0c5d1f1edd"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000\n",
    "MAX_SEQUENCE_LENGTH = 400\n",
    "VALIDATION_SAMPLES = 5000\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 512\n",
    "MAX_TAGS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba5a1b8109dee2c9fbc628d5da4a7c3447d42fb8"
   },
   "outputs": [],
   "source": [
    "with open(EMB_BASE + 'word_index','r') as f:\n",
    "    word_index = json.load(f)\n",
    "print(word_index.get(\"python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5b3ca6bd13ef2de22e1f48f0ddc59ff26cefb2e"
   },
   "outputs": [],
   "source": [
    "def do_stuff(row):\n",
    "    try:\n",
    "        return row.tags.split('|')\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "df['tagslist'] = df.apply(lambda row: do_stuff(row), axis=1)\n",
    "label_counts = {}\n",
    " \n",
    "for tags in df['tagslist']:\n",
    "    for tag in tags:\n",
    "        try:\n",
    "            label_counts[tag] += 1\n",
    "        except:\n",
    "            label_counts[tag] = 1\n",
    "all_sorted = sorted(label_counts.items(),key=lambda x : x[1],reverse=True)\n",
    "topk = all_sorted[MAX_TAGS*5:MAX_TAGS*6]\n",
    "topk = dict(topk)\n",
    "total_all = 0\n",
    "for key,val in all_sorted:\n",
    "    total_all += val\n",
    "total_k = 0\n",
    "for key,val in topk.items():\n",
    "    total_k += val\n",
    "total_k/total_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42dda8b1d420a5bc6ebb3fc76b72ae7af55664f0"
   },
   "outputs": [],
   "source": [
    "def do_stuff(row):\n",
    "    tags = []\n",
    "    for tag in row.tagslist:\n",
    "        if tag in topk.keys():\n",
    "            tags.append(tag)\n",
    "    return list(set(tags))            \n",
    "    \n",
    "df['new_tagslist'] = df.apply(lambda row: do_stuff(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "974ded703c33e5a125cbc08d2567ba1873c641b0"
   },
   "outputs": [],
   "source": [
    "label_index = {}\n",
    "i = 0\n",
    "for tags in df['new_tagslist'].values:\n",
    "    for tag in tags:\n",
    "        try:\n",
    "            label_index[tag]\n",
    "        except:\n",
    "            label_index[tag] = i\n",
    "            i += 1\n",
    "label_index.get('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2dbd961b9c33b68e1f5611c2fa186194737f2f83"
   },
   "outputs": [],
   "source": [
    "with open('label_index6','w') as f:\n",
    "    json.dump(label_index,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2865eb80d55b15f2cd64e74578e5aee98b6baaa3"
   },
   "outputs": [],
   "source": [
    "def datagen(df,batch_size=128,sequence_length=100):\n",
    "    x,y = [],[]\n",
    "    while True:\n",
    "        i = 0\n",
    "        while i < len(df):\n",
    "            if len(x) < batch_size:\n",
    "                seq = []\n",
    "                for word in df['texts'][i].split():\n",
    "                    if word_index.get(word.lower()):\n",
    "                        seq.append(word_index.get(word.lower()))\n",
    "                for _ in range(sequence_length):\n",
    "                    seq.append(0)\n",
    "                if len(seq) > sequence_length:\n",
    "                    seq = seq[:sequence_length]\n",
    "                label = []\n",
    "                for tag in df['new_tagslist'][i]:\n",
    "                    label.append(label_index.get(tag.lower()))\n",
    "                label_vec = np.zeros(len(label_index))\n",
    "                for l in label:\n",
    "                    label_vec[l] += 1\n",
    "                x.append(seq)\n",
    "                y.append(label_vec)\n",
    "            else:\n",
    "                yield np.array(x),np.array(y)\n",
    "                x,y = [],[]\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "acace89e22f3b2a2868ad288a462700e7ac4f11b"
   },
   "outputs": [],
   "source": [
    "train_df,val_df = train_test_split(df,test_size=VALIDATION_SAMPLES,random_state=31)\n",
    "train_gen = datagen(train_df.reset_index(drop=True),batch_size=BATCH_SIZE,sequence_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c06ff4d85b3942db1d4e15a003f094e50dc68579"
   },
   "outputs": [],
   "source": [
    "valid_gen = datagen(val_df.reset_index(drop=True),batch_size=len(val_df),sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "valid_x, valid_y = [], []\n",
    "for x,y in valid_gen:\n",
    "    valid_x.extend(list(x))\n",
    "    valid_y.extend(list(y))\n",
    "    break\n",
    "valid_x = np.array(valid_x)\n",
    "valid_y = np.array(valid_y)\n",
    "valid_x.shape,valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1660ea45763dc7096f58074d1c12c6494c7bed04"
   },
   "outputs": [],
   "source": [
    "test_x = []\n",
    "for i in tqdm(range(len(df_test))):\n",
    "    seq = []\n",
    "    for word in df_test['texts'][i].split():\n",
    "        if word_index.get(word.lower()):\n",
    "            seq.append(word_index.get(word.lower()))\n",
    "    for _ in range(MAX_SEQUENCE_LENGTH):\n",
    "        seq.append(0)\n",
    "    if len(seq) > MAX_SEQUENCE_LENGTH:\n",
    "        seq = seq[:MAX_SEQUENCE_LENGTH]\n",
    "    test_x.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "5c5d681b532870620aa9e756bf9d65dda4b26ed7"
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "#https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d50ffe7896ecdcbdfc3bc931ca56eebd162ea04e"
   },
   "outputs": [],
   "source": [
    "#https://github.com/arthurdouillard/keras-snapshot_ensembles/blob/master/snapshot.py\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class Snapshot(Callback):\n",
    "\n",
    "    def __init__(self, folder_path, nb_epochs, nb_cycles=5, verbose=0):\n",
    "        if nb_cycles > nb_epochs:\n",
    "            raise ValueError('nb_epochs has to be lower than nb_cycles.')\n",
    "\n",
    "        super(Snapshot, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.folder_path = folder_path\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.nb_cycles = nb_cycles\n",
    "        self.period = self.nb_epochs // self.nb_cycles\n",
    "        self.nb_digits = len(str(self.nb_cycles))\n",
    "        self.path_format = os.path.join(self.folder_path, 'weights_cycle_{}.h5')\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch == 0 or (epoch + 1) % self.period != 0: return\n",
    "        # Only save at the end of a cycle, a not at the beginning\n",
    "\n",
    "        if not os.path.exists(self.folder_path):\n",
    "            os.makedirs(self.folder_path)\n",
    "\n",
    "        cycle = int(epoch / self.period)\n",
    "        cycle_str = str(cycle).rjust(self.nb_digits, '0')\n",
    "        self.model.save_weights(self.path_format.format(cycle_str), overwrite=True)\n",
    "\n",
    "        # Resetting the learning rate\n",
    "        K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: Reached %d-th cycle, saving model.' % (epoch, cycle))\n",
    "\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch <= 0: return\n",
    "\n",
    "        lr = self.schedule(epoch)\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: Snapchot modifying learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "\n",
    "        # Get initial learning rate\n",
    "        self.base_lr = float(K.get_value(self.model.optimizer.lr))\n",
    "\n",
    "\n",
    "    def schedule(self, epoch):\n",
    "        lr = math.pi * (epoch % self.period) / self.period\n",
    "        lr = self.base_lr / 2 * (math.cos(lr) + 1)\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0858a2928a259f34fdb1205a9b4558a92f21d307"
   },
   "outputs": [],
   "source": [
    "def get_model(emb_w = None,input_layer = None):\n",
    "    inp = input_layer if input_layer is not None else Input(shape=(MAX_SEQUENCE_LENGTH,)) \n",
    "    if emb_w is None:\n",
    "        emb = Embedding(MAX_NB_WORDS,EMBEDDING_DIM)(inp)\n",
    "    else:\n",
    "        emb = Embedding(MAX_NB_WORDS,emb_w.shape[1],weights = [emb_w])(inp)\n",
    "    emb = SpatialDropout1D(0.2)(emb)\n",
    "    x = CuDNNGRU(len(label_index)//2, return_sequences=True)(emb)\n",
    "    x_avg = GlobalMaxPooling1D()(x)\n",
    "    y = CuDNNLSTM(len(label_index)//2, return_sequences=True)(emb)\n",
    "    y_avg = GlobalMaxPooling1D()(y)\n",
    "    p = Concatenate(axis=1)([x_avg,y_avg]) \n",
    "    x = Attention(MAX_SEQUENCE_LENGTH)(x)\n",
    "    y = Attention(MAX_SEQUENCE_LENGTH)(y)\n",
    "    q = Concatenate(axis=1)([x,y])\n",
    "    z = Concatenate(axis=-1)([p,q])\n",
    "    z = Dropout(0.15)(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dense(len(label_index), activation=\"relu\")(z)\n",
    "    z = Dropout(0.15)(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dense(len(label_index), activation=\"sigmoid\")(z)\n",
    "    model = Model(inputs=inp, outputs=z)\n",
    "    return model\n",
    "\n",
    "#https://github.com/arthurdouillard/keras-snapshot_ensembles/blob/master/example.py\n",
    "def load_ensemble(folder, keep_last=None):\n",
    "    paths = glob.glob(os.path.join(folder, 'weights_cycle_*.h5'))\n",
    "    print('Found:', ', '.join(paths))\n",
    "    if keep_last is not None:\n",
    "        paths = sorted(paths)[-keep_last:]\n",
    "    print('Loading:', ', '.join(paths))\n",
    "\n",
    "    x_in = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    outputs = []\n",
    "\n",
    "    for i, path in enumerate(paths):\n",
    "        m = get_model(input_layer=x_in)\n",
    "        m.load_weights(path)\n",
    "        outputs.append(m.output)\n",
    "\n",
    "    shape = outputs[0].get_shape().as_list()\n",
    "    x = Lambda(lambda x: K.mean(K.stack(x, axis=0), axis=0),\n",
    "               output_shape=lambda _: shape)(outputs)\n",
    "    model = Model(inputs=x_in, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "b277f7827b9d4b9655bee950125f884ada746c46"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = EMB_BASE + 'matrix_glove'\n",
    "with open(EMBEDDING_FILE, 'rb') as f:\n",
    "    embedding_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4897b03a9ae5bb5ad8398b1679655fbc64242f4a"
   },
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'))\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'))\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'))\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'))\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "def dice_coef(y_true, y_pred, smooth=0):\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    union = K.sum(y_true) + K.sum(y_pred)\n",
    "    return K.mean( (2. * intersection + smooth) / (union + smooth))\n",
    "def dice_p_bce(in_gt, in_pred):\n",
    "    return binary_crossentropy(in_gt, in_pred) - 0.1*dice_coef(in_gt, in_pred,smooth=1)\n",
    "\n",
    "model = get_model(emb_w=embedding_matrix)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=dice_p_bce, optimizer=RMSprop(lr=2e-3), metrics=[f1])\n",
    "snapshot = Snapshot('snapshots6', nb_epochs=12, verbose=1, nb_cycles=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88d02d09f4ee6a0d903cecaf7903cb0895432cfd"
   },
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=len(train_df)//BATCH_SIZE,\n",
    "    epochs=4,\n",
    "    verbose=1,\n",
    "    validation_data=(valid_x,valid_y),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c0010e518288bc7f588776c58610949140a139a"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=dice_p_bce, optimizer=RMSprop(lr=0.0015), metrics=[f1])\n",
    "model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=len(train_df)//BATCH_SIZE,\n",
    "    epochs=12,\n",
    "    verbose=1,\n",
    "    callbacks=[snapshot],\n",
    "    validation_data=(valid_x,valid_y),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce016ae506ad2a27062972ffcb26f54cc2345087"
   },
   "outputs": [],
   "source": [
    "model = load_ensemble('snapshots6')\n",
    "valid_preds = model.predict(valid_x,verbose=1)\n",
    "thresholds = np.linspace(0,1,200)\n",
    "max_f1 = -1\n",
    "best_thres = -1\n",
    "for thres in tqdm(thresholds):\n",
    "    f1 = f1_score(valid_y,valid_preds>thres,average='micro')\n",
    "    if f1 > max_f1:\n",
    "        max_f1 = f1\n",
    "        best_thres = thres\n",
    "print(max_f1,best_thres)\n",
    "pred_test_y = model.predict([test_x], batch_size=256, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8612aa99045d0c80bc9c8adb90c7df2d28c0381c"
   },
   "outputs": [],
   "source": [
    "pred_test_y = (pred_test_y>best_thres).astype(int)\n",
    "label_index_inverse = {val:key for key,val in label_index.items()}\n",
    "def vector_to_tags(vec):\n",
    "    tags = []\n",
    "    for i,v in enumerate(vec):\n",
    "        if v == 1:\n",
    "            tags.append(label_index_inverse[i])\n",
    "    return '|'.join(tags)\n",
    "vector_to_tags(pred_test_y[2]>best_thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "42a781d775fdf4d19d077ea659d8b0f46204a689"
   },
   "outputs": [],
   "source": [
    "df_test['tags'] = [vector_to_tags(pred>best_thres) for pred in tqdm(pred_test_y)]\n",
    "df_test.drop(columns=['texts'],inplace=True)\n",
    "df_test.to_csv('submission6.csv',index=False)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c768246cba66205b6b0aaced4d381d212fb3fefd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
